{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOD5baC9Pz39",
        "outputId": "6e4638d1-f6d1-479a-ffc4-e6c33fce78db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: arxiv in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (2.3.0)\n",
            "Requirement already satisfied: requests in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (2.32.5)\n",
            "Requirement already satisfied: psutil in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (7.1.0)\n",
            "Requirement already satisfied: memory_profiler in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (0.61.0)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (from arxiv) (6.0.12)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: sgmllib3k in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install arxiv requests psutil memory_profiler\n",
        "# !apt-get install -y file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0l5xvk3P7el",
        "outputId": "670bd2da-6e06-4830-fba0-a4e53b79ccd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing arxiv_crawler.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile arxiv_crawler.py\n",
        "import arxiv\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import tarfile\n",
        "import shutil\n",
        "import subprocess\n",
        "import gzip\n",
        "\n",
        "SAVE_DIR = \"./ArXivPapers\"\n",
        "\n",
        "\n",
        "def detect_and_fix_filetype(tar_path):\n",
        "    try:\n",
        "        result = subprocess.run([\"file\", tar_path], capture_output=True, text=True, errors='ignore')\n",
        "        output = result.stdout.strip()\n",
        "    except FileNotFoundError:\n",
        "        print(\"X 'file' command not found. Install 'file' utility.\")\n",
        "        return \"unknown\", None\n",
        "    except Exception as e:\n",
        "        print(f\"X Error running 'file': {e}\")\n",
        "        return \"unknown\", None\n",
        "\n",
        "    if \"PDF document\" in output:\n",
        "        print(f\"  -> Detected PDF: {os.path.basename(tar_path)}\")\n",
        "        return \"pdf\", None\n",
        "    elif \"gzip compressed data\" in output:\n",
        "        match = re.search(r', was \"([^\"]+)\"', output)\n",
        "        if match:\n",
        "            return \"gz\", os.path.basename(match.group(1))\n",
        "        else:\n",
        "            return \"tar.gz\", None\n",
        "    elif \"tar archive\" in output:\n",
        "        return \"tar.gz\", None\n",
        "    else:\n",
        "        print(f\"  Unknown format: {output}\")\n",
        "        return \"unknown\", None\n",
        "\n",
        "\n",
        "def extract_and_clean(tar_path, dest_folder, base_name):\n",
        "    filetype, orig_name = detect_and_fix_filetype(tar_path)\n",
        "    extract_path = os.path.join(dest_folder, base_name)\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "    deleted = 0\n",
        "\n",
        "    if filetype == \"pdf\":\n",
        "        return (True, 0)\n",
        "    if filetype == \"unknown\":\n",
        "        return (False, 0)\n",
        "\n",
        "    try:\n",
        "        if filetype == \"tar.gz\":\n",
        "            with tarfile.open(tar_path, 'r:*') as tar:\n",
        "                tar.extractall(path=extract_path)\n",
        "        elif filetype == \"gz\":\n",
        "            out_name = orig_name or f\"{base_name}.file\"\n",
        "            out_path = os.path.join(extract_path, out_name)\n",
        "            with gzip.open(tar_path, 'rb') as fin, open(out_path, 'wb') as fout:\n",
        "                shutil.copyfileobj(fin, fout)\n",
        "    except Exception as e:\n",
        "        print(f\"X Extract error: {e}\")\n",
        "        shutil.rmtree(extract_path, ignore_errors=True)\n",
        "        return (False, 0)\n",
        "\n",
        "    # Clean: keep only .tex and .bib\n",
        "    for root, _, files in os.walk(extract_path):\n",
        "        for f in files:\n",
        "            if not f.lower().endswith(('.tex', '.bib')):\n",
        "                try:\n",
        "                    os.remove(os.path.join(root, f))\n",
        "                    deleted += 1\n",
        "                except:\n",
        "                    pass\n",
        "    return (True, deleted)\n",
        "\n",
        "\n",
        "def crawl_single_paper(arxiv_id, save_dir=SAVE_DIR):\n",
        "    \"\"\"\n",
        "    Download and process a single arXiv paper with all its versions.\n",
        "\n",
        "    Args:\n",
        "        arxiv_id: arXiv ID in format yymm.nnnnn (e.g., \"2305.04793\")\n",
        "        save_dir: Directory to save the paper data\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful, False otherwise\n",
        "    \"\"\"\n",
        "    client = arxiv.Client()\n",
        "    paper_folder = None\n",
        "    tex_folder = None\n",
        "    versions_processed = 0\n",
        "    latest_version = 0\n",
        "\n",
        "    # Validate and split ID\n",
        "    if '.' not in arxiv_id:\n",
        "        print(f\"X Invalid arxiv_id: {arxiv_id}\")\n",
        "        return False\n",
        "\n",
        "    prefix, suffix = arxiv_id.split('.')\n",
        "    paper_folder = os.path.join(save_dir, f\"{prefix}-{suffix}\")\n",
        "    tex_folder = os.path.join(paper_folder, \"tex\")\n",
        "    os.makedirs(tex_folder, exist_ok=True)\n",
        "\n",
        "    # Get latest version from v1\n",
        "    try:\n",
        "        search = arxiv.Search(id_list=[arxiv_id])\n",
        "        base_paper = next(client.results(search))\n",
        "        match = re.search(r'v(\\d+)$', base_paper.entry_id)\n",
        "        latest_version = int(match.group(1)) if match else 1\n",
        "        print(f\"[{arxiv_id}] Found {latest_version} version(s)\")\n",
        "    except StopIteration:\n",
        "        print(f\"X [{arxiv_id}] Paper not found\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"X [{arxiv_id}] Error finding latest version: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Collect metadata from v1\n",
        "    title = base_paper.title\n",
        "    authors = [a.name for a in base_paper.authors]\n",
        "    submission_date = base_paper.published.strftime(\"%Y-%m-%d\") if base_paper.published else None\n",
        "    publication_venue = base_paper.journal_ref if base_paper.journal_ref else None\n",
        "    categories = base_paper.categories\n",
        "    revised_dates = []\n",
        "\n",
        "    # Get revised dates for v2..vN\n",
        "    if latest_version > 1:\n",
        "        for v in range(2, latest_version + 1):\n",
        "            try:\n",
        "                vid = f\"{arxiv_id}v{v}\"\n",
        "                search_v = arxiv.Search(id_list=[vid])\n",
        "                paper_v = next(client.results(search_v))\n",
        "                revised_dates.append(paper_v.updated.strftime(\"%Y-%m-%d\") if paper_v.updated else None)\n",
        "            except:\n",
        "                revised_dates.append(None)\n",
        "\n",
        "    # Save metadata.json\n",
        "    metadata = {\n",
        "        \"arxiv_id\": arxiv_id.replace('.', '-'),\n",
        "        \"paper_title\": title,\n",
        "        \"authors\": authors,\n",
        "        \"submission_date\": submission_date,\n",
        "        \"revised_dates\": revised_dates,\n",
        "        \"publication_venue\": publication_venue,\n",
        "        \"latest_version\": latest_version,\n",
        "        \"categories\": categories\n",
        "    }\n",
        "\n",
        "    metadata_path = os.path.join(paper_folder, \"metadata.json\")\n",
        "    try:\n",
        "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(metadata, f, indent=4, ensure_ascii=False)\n",
        "        print(f\"  [{arxiv_id}] Saved metadata.json\")\n",
        "    except Exception as e:\n",
        "        print(f\"X [{arxiv_id}] Failed to save metadata: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Download all versions into tex folder\n",
        "    for v in range(1, latest_version + 1):\n",
        "        version_id = f\"{arxiv_id}v{v}\"\n",
        "        version_folder_name = f\"{prefix}-{suffix}v{v}\"\n",
        "        temp_tar = os.path.join(paper_folder, f\"{version_id}.tar.gz\")\n",
        "\n",
        "        try:\n",
        "            search_v = arxiv.Search(id_list=[version_id])\n",
        "            paper_v = next(client.results(search_v))\n",
        "\n",
        "            print(f\"  [{arxiv_id}] Downloading {version_id}...\")\n",
        "            paper_v.download_source(dirpath=paper_folder, filename=f\"{version_id}.tar.gz\")\n",
        "\n",
        "            # Extract & Clean into tex folder\n",
        "            success, deleted_count = extract_and_clean(temp_tar, tex_folder, version_folder_name)\n",
        "\n",
        "            if success:\n",
        "                versions_processed += 1\n",
        "                print(f\"  [{arxiv_id}] Extracted & cleaned: {version_id} ({deleted_count} files removed)\")\n",
        "            else:\n",
        "                print(f\"X [{arxiv_id}] Failed to extract {version_id}\")\n",
        "\n",
        "            # Delete .tar.gz\n",
        "            try:\n",
        "                os.remove(temp_tar)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            time.sleep(0.5)  # Be nice to arXiv\n",
        "\n",
        "        except StopIteration:\n",
        "            print(f\"X [{arxiv_id}] Version {version_id} not found\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"X [{arxiv_id}] Download error {version_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Final check\n",
        "    success = (versions_processed > 0)\n",
        "    if success:\n",
        "        print(f\"✓ [{arxiv_id}] COMPLETED ({versions_processed}/{latest_version} versions)\")\n",
        "    else:\n",
        "        print(f\"X [{arxiv_id}] FAILED - no versions downloaded\")\n",
        "\n",
        "    return success\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QApGhns2P8U8",
        "outputId": "8636af08-5546-4b71-b94e-aadeb1968078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing reference_extractor.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile reference_extractor.py\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "\n",
        "def format_arxiv_id_for_key(arxiv_id):\n",
        "    \"\"\"\n",
        "    Convert arXiv ID to folder format (yymm-nnnnn).\n",
        "    Examples:\n",
        "        \"2305.04793\" -> \"2305-04793\"\n",
        "        \"2305.04793v1\" -> \"2305-04793\"\n",
        "    \"\"\"\n",
        "    # Remove version suffix if present\n",
        "    clean_id = re.sub(r'v\\d+$', '', arxiv_id)\n",
        "    # Replace dot with dash\n",
        "    return clean_id.replace('.', '-')\n",
        "\n",
        "\n",
        "def get_paper_references(arxiv_id, delay=3):\n",
        "    \"\"\"\n",
        "    Fetch references for a paper from Semantic Scholar API.\n",
        "    Retries indefinitely until success or 404.\n",
        "\n",
        "    Args:\n",
        "        arxiv_id: arXiv ID (format: YYMM.NNNNN or YYMM.NNNNNvN)\n",
        "        delay: delay between retries in seconds\n",
        "\n",
        "    Returns:\n",
        "        tuple: (list of references, total_found_count) or (None, 0) if 404 error\n",
        "    \"\"\"\n",
        "    # Clean arxiv_id (remove version suffix if present)\n",
        "    clean_id = re.sub(r'v\\d+$', '', arxiv_id)\n",
        "    url = f\"https://api.semanticscholar.org/graph/v1/paper/arXiv:{clean_id}\"\n",
        "    params = {\n",
        "        \"fields\": \"references,references.title,references.authors,references.year,references.venue,references.externalIds,references.publicationDate\"\n",
        "    }\n",
        "\n",
        "    ### My SEMANTIC_SCHOLAR_API_KEY\n",
        "    # Can use one of these keys behind\n",
        "    API_KEY = os.getenv(\"a8okwqTLp18Ku1vBXJ1Jb6eRoDKpmAem41VjtFCY\")\n",
        "    # API_KEY = os.getenv(\"cf6G5yldwF4UEzswq3WKX72B6uffqNv17LQDo8Oi\")\n",
        "    headers = {}\n",
        "    if API_KEY:\n",
        "        headers[\"x-api-key\"] = API_KEY\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            #response = requests.get(url, params=params, timeout=10)\n",
        "            response = requests.get(url, params=params, headers=headers, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                references = data.get(\"references\", [])\n",
        "                total_found = len(references) if references else 0\n",
        "                return references, total_found\n",
        "            elif response.status_code == 429:\n",
        "                print(f\"  [{arxiv_id}] Rate limit hit. Waiting {delay}s...\")\n",
        "                time.sleep(delay)\n",
        "            elif response.status_code == 404:\n",
        "                print(f\"  [{arxiv_id}] Paper not found in Semantic Scholar (404)\")\n",
        "                return None, 0  # Return None to indicate 404 error\n",
        "            else:\n",
        "                print(f\"  [{arxiv_id}] API returned status {response.status_code}, retrying in {delay}s...\")\n",
        "                time.sleep(delay)\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"  [{arxiv_id}] Request error: {e}, retrying in {delay}s...\")\n",
        "            time.sleep(delay)\n",
        "\n",
        "\n",
        "def convert_to_references_dict(references):\n",
        "    \"\"\"\n",
        "    Convert Semantic Scholar references to the required format:\n",
        "    Dictionary with arXiv IDs as keys (in \"yyyymm-id\" format) for papers with arXiv IDs.\n",
        "\n",
        "    Args:\n",
        "        references: List of references from Semantic Scholar API\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with paper IDs as keys and metadata as values\n",
        "    \"\"\"\n",
        "    result = {}\n",
        "\n",
        "    for ref in references:\n",
        "        # Skip if reference is None or empty\n",
        "        if not ref:\n",
        "            continue\n",
        "\n",
        "        # Extract external IDs (may be None)\n",
        "        external_ids = ref.get(\"externalIds\", {})\n",
        "        if external_ids is None:\n",
        "            external_ids = {}\n",
        "\n",
        "        arxiv_id = external_ids.get(\"ArXiv\", \"\")\n",
        "\n",
        "        # Only keep references that have arXiv_id\n",
        "        if not arxiv_id:\n",
        "            continue\n",
        "\n",
        "        # Use arXiv ID in yyyymm-id format\n",
        "        key = format_arxiv_id_for_key(arxiv_id)\n",
        "\n",
        "        # Extract authors\n",
        "        authors_list = ref.get(\"authors\", [])\n",
        "        authors = [author.get(\"name\", \"\") for author in authors_list if author.get(\"name\")]\n",
        "\n",
        "        # Extract dates (use publicationDate if available)\n",
        "        publication_date = ref.get(\"publicationDate\", \"\")\n",
        "        year = ref.get(\"year\")\n",
        "\n",
        "        # If no publication date but have year, create an ISO-like format\n",
        "        if not publication_date and year:\n",
        "            publication_date = f\"{year}-01-01\"  # Use Jan 1st as placeholder\n",
        "\n",
        "        # Build metadata dictionary with required fields\n",
        "        metadata = {\n",
        "            \"paper_title\": ref.get(\"title\", \"\"),\n",
        "            \"authors\": authors,\n",
        "            \"submission_date\": publication_date if publication_date else \"\",\n",
        "            \"semantic_scholar_id\": ref.get(\"paperId\")\n",
        "        }\n",
        "\n",
        "        result[key] = metadata\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def extract_references_for_paper(arxiv_id, save_dir=\"./ArXivPapers\"):\n",
        "    \"\"\"\n",
        "    Extract references for a paper and save to references.json.\n",
        "\n",
        "    Args:\n",
        "        arxiv_id: arXiv ID in format yymm.nnnnn (e.g., \"2305.04793\")\n",
        "        save_dir: Base directory containing paper folders\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful (found and saved references), False otherwise\n",
        "    \"\"\"\n",
        "    # Convert arxiv_id to folder format\n",
        "    paper_id_key = format_arxiv_id_for_key(arxiv_id)\n",
        "    paper_folder = os.path.join(save_dir, paper_id_key)\n",
        "\n",
        "    # Check if the folder exists\n",
        "    if not os.path.exists(paper_folder):\n",
        "        print(f\"X [{arxiv_id}] Paper folder not found: {paper_folder}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"[{arxiv_id}] Fetching references...\")\n",
        "\n",
        "    try:\n",
        "        json_path = os.path.join(paper_folder, \"references.json\")\n",
        "        references, total_found = get_paper_references(arxiv_id)\n",
        "\n",
        "        # If we got None (404 error), save empty file and return failure\n",
        "        if references is None:\n",
        "            print(f\"X [{arxiv_id}] Failed to fetch references from Semantic Scholar (404)\")\n",
        "            with open(json_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump({}, f, indent=4, ensure_ascii=False)\n",
        "            return False\n",
        "\n",
        "        if not references or total_found == 0:\n",
        "            print(f\"X [{arxiv_id}] No references found (total_found: 0)\")\n",
        "            # Save empty dict but return False\n",
        "            with open(json_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump({}, f, indent=4, ensure_ascii=False)\n",
        "            return False\n",
        "\n",
        "        references_dict = convert_to_references_dict(references)\n",
        "        total_saved = len(references_dict)\n",
        "\n",
        "        # Save only the references dict (no statistics in JSON)\n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(references_dict, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        # Log statistics to console only\n",
        "        print(f\"✓ [{arxiv_id}] Found {total_found} references, saved {total_saved} (with arXiv IDs) to references.json\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"X [{arxiv_id}] Error extracting references: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoKzTzp_P-V3",
        "outputId": "554e0335-1cff-41b5-c968-3e0888ae3e8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import psutil\n",
        "import threading\n",
        "import sys\n",
        "import requests\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from threading import Lock\n",
        "from arxiv_crawler import crawl_single_paper\n",
        "from reference_extractor import extract_references_for_paper\n",
        "\n",
        "\n",
        "# Global statistics\n",
        "stats_lock = Lock()\n",
        "stats = {\n",
        "    \"total_processed\": 0,           # Total number of papers processed\n",
        "    \"both_success\": 0,              # Both 2 parts are successful\n",
        "    \"only_crawler_fail\": 0,         # Only fail crawlers\n",
        "    \"no_references\": 0,             # 404 - No found or 0 reference\n",
        "    \"both_failed\": 0,               # Both 2 parts failed\n",
        "}\n",
        "\n",
        "monitor_running = True\n",
        "ram_samples_bytes = []\n",
        "peak_disk_usage_bytes = 0\n",
        "\n",
        "def _monitor_resources(baseline_ram, baseline_disk, sleep_interval=2):\n",
        "    \"\"\"\n",
        "    Runs in the background to monitor average RAM and peak Disk.\n",
        "    \"\"\"\n",
        "    global ram_samples_bytes, peak_disk_usage_bytes, monitor_running\n",
        "\n",
        "    # Reset\n",
        "    ram_samples_bytes = []\n",
        "    peak_disk_usage_bytes = 0\n",
        "\n",
        "    print(f\"[Monitor] Start (Baseline RAM: {baseline_ram / (1024**3):.2f} GB, Baseline Disk: {baseline_disk / (1024**3):.2f} GB)\")\n",
        "\n",
        "    while monitor_running:\n",
        "        try:\n",
        "            # RAM for all system\n",
        "            current_ram = psutil.virtual_memory().used\n",
        "            ram_above_baseline = current_ram - baseline_ram\n",
        "            ram_samples_bytes.append(ram_above_baseline)\n",
        "\n",
        "            # Disk\n",
        "            current_disk = shutil.disk_usage('/').used\n",
        "            disk_above_baseline = current_disk - baseline_disk\n",
        "\n",
        "            if disk_above_baseline > peak_disk_usage_bytes:\n",
        "                peak_disk_usage_bytes = disk_above_baseline\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[Monitor Error] {e}\")\n",
        "\n",
        "        time.sleep(sleep_interval)\n",
        "\n",
        "    print(\"[Monitor] Stop.\")\n",
        "\n",
        "def _print_custom_resource_report(disk_start, disk_end):\n",
        "    \"\"\"\n",
        "    Print reports for missing requests\n",
        "    \"\"\"\n",
        "    global ram_samples_bytes, peak_disk_usage_bytes\n",
        "\n",
        "    # RAM for all system\n",
        "    avg_ram_bytes = 0\n",
        "    peak_ram_bytes = 0\n",
        "    if ram_samples_bytes:\n",
        "        avg_ram_bytes = sum(ram_samples_bytes) / len(ram_samples_bytes)\n",
        "        peak_ram_bytes = max(ram_samples_bytes)\n",
        "\n",
        "    # The Last Disk\n",
        "    final_disk_bytes = disk_end - disk_start\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ADDITIONAL RESOURCE REPORT (MINUS BASELINE)\")\n",
        "\n",
        "    print(f\"  Average RAM for all system : {avg_ram_bytes / (1024**2):.2f} MB\")\n",
        "    print(f\"  Peak RAM for all system    : {peak_ram_bytes / (1024**2):.2f} MB\")\n",
        "    print(f\"  Peak Disk for all system   : {peak_disk_usage_bytes / (1024**2):.2f} MB\")\n",
        "    print(f\"  Final Disk for all system  : {final_disk_bytes / (1024**2):.2f} MB\")\n",
        "\n",
        "    print(\"=\"*80)\n",
        "\n",
        "def process_paper(arxiv_id, save_dir=\"./ArXivPapers\"):\n",
        "    \"\"\"\n",
        "    Process a single paper: crawl data first, then extract references.\n",
        "\n",
        "    Args:\n",
        "        arxiv_id: arXiv ID in format yymm.nnnnn\n",
        "        save_dir: Directory to save data\n",
        "\n",
        "    Returns:\n",
        "        tuple: (arxiv_id, crawler_success, references_success)\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Processing paper: {arxiv_id}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Step 1: Crawl paper data\n",
        "    crawler_success = crawl_single_paper(arxiv_id, save_dir)\n",
        "\n",
        "    # Step 2: Extract references (only if crawler succeeded)\n",
        "    references_success = extract_references_for_paper(arxiv_id, save_dir)\n",
        "\n",
        "    # Update statistics\n",
        "    with stats_lock:\n",
        "        stats[\"total_processed\"] += 1\n",
        "\n",
        "        if crawler_success:\n",
        "            stats[\"both_success\"] += 1\n",
        "        elif not crawler_success:\n",
        "            stats[\"only_crawler_fail\"] += 1\n",
        "\n",
        "        if not references_success:\n",
        "            stats[\"no_references\"] += 1\n",
        "\n",
        "    return arxiv_id, crawler_success, references_success\n",
        "\n",
        "\n",
        "def check_paper_exists(arxiv_id):\n",
        "    \"\"\"\n",
        "    Check if paper exists with a very lightweight HEAD request\n",
        "    \"\"\"\n",
        "    url = f\"https://arxiv.org/abs/{arxiv_id}\"\n",
        "    try:\n",
        "        response = requests.head(url, timeout=5, allow_redirects=True)\n",
        "        time.sleep(0.5)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            return True\n",
        "        elif response.status_code == 404:\n",
        "            return False\n",
        "        else:\n",
        "            print(f\"  [Warning] Get status code {response.status_code} for {arxiv_id}\")\n",
        "            return False\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"  [Error] Request error when checking {arxiv_id}: {e}\")\n",
        "        time.sleep(0.5) # be nice to arXiv\n",
        "        return False\n",
        "\n",
        "\n",
        "def find_last_valid_id(prefix, start_id, jump1=50, back1=10, jump2=5, back2=1):\n",
        "    \"\"\"\n",
        "    Find the last valid ID using a multi-step strategy\n",
        "    Logic: JUMP1 -> (fail) -> BACK1 -> (success) -> JUMP2 -> (fail) -> BACK2 -> (success) = DONE\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Start multi-step search for {prefix}.xxxxx (Starting from: {start_id})\")\n",
        "    print(f\"Strategy: JUMP1={jump1}, BACK1={back1}, JUMP2={jump2}, BACK2={back2}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    try:\n",
        "        start_id = int(start_id)\n",
        "        jump1 = int(jump1)\n",
        "        back1 = int(back1)\n",
        "        jump2 = int(jump2)\n",
        "        back2 = int(back2)\n",
        "    except ValueError:\n",
        "        print(f\"Error: One of the parameters is not a number\")\n",
        "        return 0\n",
        "\n",
        "    start_arxiv_id = f\"{prefix}.{start_id:05d}\"\n",
        "    print(f\"Probing (start): {start_arxiv_id}\", end=\"\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    if not check_paper_exists(start_arxiv_id):\n",
        "        print(\" ... X Not exist\")\n",
        "        print(f\"Error: Start ID {start_arxiv_id} does not exist. Cannot be found\")\n",
        "        return start_id - 1\n",
        "\n",
        "    print(\" ... ✓ Exist\")\n",
        "\n",
        "    last_known_good_id = start_id\n",
        "    current_id = start_id + jump1\n",
        "    state = \"JUMP1\"\n",
        "\n",
        "    while True:\n",
        "        arxiv_id = f\"{prefix}.{current_id:05d}\"\n",
        "\n",
        "        print(f\"Probing ({state}): {arxiv_id}\", end=\"\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        exists = check_paper_exists(arxiv_id)\n",
        "\n",
        "        if exists:\n",
        "            print(\" ... ✓ Exist\")\n",
        "            last_known_good_id = current_id\n",
        "\n",
        "            if state == \"JUMP1\":\n",
        "                current_id += jump1\n",
        "            elif state == \"BACK1\":\n",
        "                print(f\" -> Switch to JUMP2\")\n",
        "                state = \"JUMP2\"\n",
        "                current_id += jump2\n",
        "            elif state == \"JUMP2\":\n",
        "                current_id += jump2\n",
        "            elif state == \"BACK2\":\n",
        "                print(f\"\\n-> ID {arxiv_id} exists after BACK2. This is the final ID!\")\n",
        "                break\n",
        "\n",
        "        else:\n",
        "            print(\" ... X Not exist\")\n",
        "\n",
        "            if state == \"JUMP1\":\n",
        "                state = \"BACK1\"\n",
        "                current_id -= back1\n",
        "            elif state == \"BACK1\":\n",
        "                current_id -= back1\n",
        "            elif state == \"JUMP2\":\n",
        "                state = \"BACK2\"\n",
        "                current_id -= back2\n",
        "            elif state == \"BACK2\":\n",
        "                current_id -= back2\n",
        "\n",
        "    last_valid_id = last_known_good_id\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Last valid ID found: {prefix}.{last_valid_id:05d}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    return last_valid_id\n",
        "\n",
        "\n",
        "def generate_paper_ids(start_month, start_id, end_month, end_id, save_dir=\"./ArXivPapers\"):\n",
        "    \"\"\"\n",
        "    Generate list of arXiv IDs based on date range.\n",
        "\n",
        "    Args:\n",
        "        start_month: Start month in format \"YYYY-MM\"\n",
        "        start_id: Starting ID number\n",
        "        end_month: End month in format \"YYYY-MM\"\n",
        "        end_id: Ending ID number\n",
        "        save_dir: Directory to save data\n",
        "\n",
        "    Returns:\n",
        "        list: List of arXiv IDs in format \"yymm.nnnnn\"\n",
        "    \"\"\"\n",
        "    start_year, start_mon = start_month.split('-')\n",
        "    end_year, end_mon = end_month.split('-')\n",
        "    start_prefix = start_year[2:] + start_mon\n",
        "    end_prefix = end_year[2:] + end_mon\n",
        "\n",
        "    paper_ids = []\n",
        "\n",
        "    if start_month == end_month:\n",
        "        # Same month - simple range\n",
        "        print(f\"Single month mode: {start_prefix}.{start_id:05d} → {start_prefix}.{end_id:05d}\")\n",
        "        for i in range(start_id, end_id + 1):\n",
        "            paper_ids.append(f\"{start_prefix}.{i:05d}\")\n",
        "    else:\n",
        "        # Different months - need to find last valid ID in start month\n",
        "        print(f\"Multi-month mode: {start_prefix}.{start_id:05d} → {end_prefix}.{end_id:05d}\")\n",
        "\n",
        "        # Find last valid ID in start month\n",
        "        last_valid_start_month = find_last_valid_id(start_prefix, start_id)\n",
        "\n",
        "        # Add papers from start month\n",
        "        for i in range(start_id, last_valid_start_month + 1):\n",
        "            paper_ids.append(f\"{start_prefix}.{i:05d}\")\n",
        "\n",
        "        # Add papers from end month (from 1 to end_id)\n",
        "        print(f\"\\nAdding papers from end month: {end_prefix}.00001 → {end_prefix}.{end_id:05d}\")\n",
        "        for i in range(1, end_id + 1):\n",
        "            paper_ids.append(f\"{end_prefix}.{i:05d}\")\n",
        "\n",
        "    return paper_ids\n",
        "\n",
        "\n",
        "def print_progress_report():\n",
        "    \"\"\"Print current statistics.\"\"\"\n",
        "    with stats_lock:\n",
        "        print(\"CURRENT PROGRESS:\")\n",
        "        print(f\"  Total processed                                                              : {stats['total_processed']}\")\n",
        "        print(f\"  Both success                                                                 : {stats['both_success']}\")\n",
        "        print(f\"  Only crawler fail                                                            : {stats['only_crawler_fail']}\")\n",
        "        print(f\"  Don't have references but it's still success (404 - No found or 0 reference) : {stats['no_references']}\")\n",
        "        print(f\"  Both failed                                                                  : {stats['both_failed']}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "\n",
        "def print_final_report():\n",
        "    \"\"\"Print final statistics with percentages.\"\"\"\n",
        "    total = stats['total_processed']\n",
        "\n",
        "    # Calculate success rates\n",
        "    both_success_rate = (stats['both_success'] / total * 100) if total > 0 else 0\n",
        "    crawl_fail_rate = (( stats['both_failed']) / total * 100) if total > 0 else 0\n",
        "    no_references_rate = (( stats['no_references']) / total * 100) if total > 0 else 0\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"FINAL REPORT:\")\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"CURRENT PROGRESS:\")\n",
        "    print(f\"  Total processed                                                              : {stats['total_processed']}\")\n",
        "    print(f\"  Both success                                                                 : {stats['both_success']}\")\n",
        "    print(f\"  Only crawler fail                                                            : {stats['only_crawler_fail']}\")\n",
        "    print(f\"  Don't have references but it's still success (404 - No found or 0 reference) : {stats['no_references']}\")\n",
        "    print(f\"  Both failed                                                                  : {stats['both_failed']}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    print(\"SUCCESS RATES:\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"  Both phases success rate  : {both_success_rate:.2f}%\")\n",
        "    print(f\"  Crawler fail              : {crawl_fail_rate:.2f}%\")\n",
        "    print(f\"  No References             : {no_references_rate:.2f}%\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "\n",
        "def run_parallel_processing(start_month, start_id, end_month, end_id,\n",
        "                            max_parallels=2, save_dir=\"./ArXivPapers\"):\n",
        "    \"\"\"\n",
        "    Main function to run parallel processing of papers.\n",
        "\n",
        "    Args:\n",
        "        start_month: Start month in format \"YYYY-MM\"\n",
        "        start_id: Starting ID number\n",
        "        end_month: End month in format \"YYYY-MM\"\n",
        "        end_id: Ending ID number\n",
        "        max_parallels: Number of parallel threads (default: 2)\n",
        "        save_dir: Directory to save data\n",
        "    \"\"\"\n",
        "    # Reset stats\n",
        "    with stats_lock:\n",
        "        for key in stats:\n",
        "            stats[key] = 0\n",
        "\n",
        "    # Generate paper IDs\n",
        "    paper_ids = generate_paper_ids(start_month, start_id, end_month, end_id, save_dir)\n",
        "    total_papers = len(paper_ids)\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"STARTING PARALLEL PROCESSING\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Range: {start_month} ID {start_id} → {end_month} ID {end_id}\")\n",
        "    print(f\"Total papers to process: {total_papers}\")\n",
        "    print(f\"Parallel threads: {max_parallels}\")\n",
        "    print(f\"Output directory: {save_dir}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Process papers in parallel\n",
        "    with ThreadPoolExecutor(max_workers=max_parallels) as executor:\n",
        "        futures = {\n",
        "            executor.submit(process_paper, arxiv_id, save_dir): arxiv_id\n",
        "            for arxiv_id in paper_ids\n",
        "        }\n",
        "\n",
        "        completed = 0\n",
        "        for future in as_completed(futures):\n",
        "            arxiv_id = futures[future]\n",
        "            completed += 1\n",
        "\n",
        "            try:\n",
        "                paper_id, crawler_ok, refs_ok = future.result()\n",
        "                status = \"✓✓\" if (crawler_ok and refs_ok) else \\\n",
        "                         \"✓X\" if (crawler_ok and not refs_ok) else \\\n",
        "                         \"XX\"\n",
        "                print(f\"\\n[{completed}/{total_papers}] {status} {paper_id}\")\n",
        "\n",
        "                # Print progress every 10 papers\n",
        "                if completed % 10 == 0:\n",
        "                    print_progress_report()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n[{completed}/{total_papers}] !! {arxiv_id} - Error: {e}\")\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Print final report with percentages\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"PROCESSING COMPLETE!\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Time elapsed: {elapsed_time:.2f} seconds\")\n",
        "    print(f\"Average time per paper: {elapsed_time/total_papers:.2f} seconds\" if total_papers > 0 else \"\")\n",
        "    print_final_report()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"The main function to run the entire processing\"\"\"\n",
        "\n",
        "    # === CONFIGS ===\n",
        "\n",
        "    START_MONTH = \"2023-04\"\n",
        "    START_ID = 14607\n",
        "    END_MONTH = \"2023-05\"\n",
        "    END_ID = 14596\n",
        "    MAX_PARALLELS = 2\n",
        "    SAVE_DIR = \"./ArXivPapers\"\n",
        "\n",
        "\n",
        "    # STEP 1: MEASURE INITIAL RESOURCES (BASELINE)\n",
        "    print(\"=\"*50)\n",
        "    print(\"Baseline resource measurement...\")\n",
        "    disk_usage_start = shutil.disk_usage('/').used\n",
        "    ram_usage_start = psutil.virtual_memory().used\n",
        "    print(f\"  Disk first - before running program : {disk_usage_start / (1024**3):.2f} GB\")\n",
        "    print(f\"  RAM first  - before running program : {ram_usage_start / (1024**3):.2f} GB\")\n",
        "\n",
        "    # STEP 2: START MONITORING THREAD\n",
        "    print(\"Start the resource monitor thread...\")\n",
        "    global monitor_running\n",
        "    monitor_running = True\n",
        "\n",
        "    monitor_thread = threading.Thread(\n",
        "        target=_monitor_resources,\n",
        "        args=(ram_usage_start, disk_usage_start, 2), # (baseline_ram, baseline_disk, 2s interval)\n",
        "        daemon=True\n",
        "    )\n",
        "    monitor_thread.start()\n",
        "\n",
        "    # STEP 3: RUN THE MAIN TASK\n",
        "    run_parallel_processing(\n",
        "        start_month=START_MONTH,\n",
        "        start_id=START_ID,\n",
        "        end_month=END_MONTH,\n",
        "        end_id=END_ID,\n",
        "        max_parallels=MAX_PARALLELS,\n",
        "        save_dir=SAVE_DIR\n",
        "    )\n",
        "\n",
        "    # STEP 4: STOP MONITORING & FINAL MEASUREMENT\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Task completed. Stopping monitoring stream...\")\n",
        "    monitor_running = False\n",
        "    monitor_thread.join() # Wait for the stream to completely shut down\n",
        "\n",
        "    disk_usage_end = shutil.disk_usage('/').used\n",
        "    print(f\"  Last disk: {disk_usage_end / (1024**3):.2f} GB\")\n",
        "\n",
        "    # STEP 5: PRINT ADDITIONAL REPORT\n",
        "    # (This function will print average RAM, peak Disk, last Disk)\n",
        "    _print_custom_resource_report(disk_usage_start, disk_usage_end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qRsP5StkQBA9",
        "outputId": "228d4284-2f42-476f-c76b-d93607ca0bb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- START MEASURING PEAK RAM OF MAIN ---\n",
            "==================================================\n",
            "Baseline resource measurement...\n",
            "  Disk first - before running program : 95.31 GB\n",
            "  RAM first  - before running program : 13.87 GB\n",
            "Start the resource monitor thread...\n",
            "[Monitor] Start (Baseline RAM: 13.87 GB, Baseline Disk: 95.31 GB)\n",
            "Multi-month mode: 2304.14607 → 2305.14596\n",
            "\n",
            "================================================================================\n",
            "Start multi-step search for 2304.xxxxx (Starting from: 14607)\n",
            "Strategy: JUMP1=50, BACK1=10, JUMP2=5, BACK2=1\n",
            "================================================================================\n",
            "Probing (start): 2304.14607 ... ✓ Exist\n",
            "Probing (JUMP1): 2304.14657 ... ✓ Exist\n",
            "Probing (JUMP1): 2304.14707 ... ✓ Exist\n",
            "Probing (JUMP1): 2304.14757 ... ✓ Exist\n",
            "Probing (JUMP1): 2304.14807 ... ✓ Exist\n",
            "Probing (JUMP1): 2304.14857 ... ✓ Exist\n",
            "Probing (JUMP1): 2304.14907 ... ✓ Exist\n",
            "Probing (JUMP1): 2304.14957 ... ✓ Exist\n",
            "Probing (JUMP1): 2304.15007 ... ✓ Exist\n",
            "Probing (JUMP1): 2304.15057 ... X Not exist\n",
            "Probing (BACK1): 2304.15047 ... X Not exist\n",
            "Probing (BACK1): 2304.15037 ... X Not exist\n",
            "Probing (BACK1): 2304.15027 ... X Not exist\n",
            "Probing (BACK1): 2304.15017 ... X Not exist\n",
            "Probing (BACK1): 2304.15007 ... ✓ Exist\n",
            " -> Switch to JUMP2\n",
            "Probing (JUMP2): 2304.15012 ... X Not exist\n",
            "Probing (BACK2): 2304.15011 ... X Not exist\n",
            "Probing (BACK2): 2304.15010 ... ✓ Exist\n",
            "\n",
            "-> ID 2304.15010 exists after BACK2. This is the final ID!\n",
            "\n",
            "================================================================================\n",
            "Last valid ID found: 2304.15010\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Adding papers from end month: 2305.00001 → 2305.14596\n",
            "\n",
            "================================================================================\n",
            "STARTING PARALLEL PROCESSING\n",
            "================================================================================\n",
            "Range: 2023-04 ID 14607 → 2023-05 ID 14596\n",
            "Total papers to process: 15000\n",
            "Parallel threads: 2\n",
            "Output directory: ./ArXivPapers\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Processing paper: 2304.14607\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Processing paper: 2304.14608\n",
            "================================================================================\n",
            "[2304.14607] Found 1 version(s)\n",
            "  [2304.14607] Saved metadata.json\n",
            "[2304.14608] Found 2 version(s)\n",
            "  [2304.14608] Saved metadata.json\n",
            "  [2304.14607] Downloading 2304.14607v1...\n",
            "  -> Detected PDF: 2304.14607v1.tar.gz\n",
            "  [2304.14607] Extracted & cleaned: 2304.14607v1 (0 files removed)\n",
            "✓ [2304.14607] COMPLETED (1/1 versions)\n",
            "[2304.14607] Fetching references...\n",
            "  [2304.14607] Rate limit hit. Waiting 3s...\n",
            "  [2304.14608] Downloading 2304.14608v1...\n",
            "✓ [2304.14607] Found 20 references, saved 0 (with arXiv IDs) to references.json\n",
            "\n",
            "================================================================================\n",
            "Processing paper: 2304.14609\n",
            "================================================================================\n",
            "\n",
            "[1/15000] ✓✓ 2304.14607\n",
            "[2304.14609] Found 3 version(s)\n",
            "  [2304.14609] Saved metadata.json\n",
            "  [2304.14608] Extracted & cleaned: 2304.14608v1 (37 files removed)\n",
            "  [2304.14608] Downloading 2304.14608v2...\n",
            "  [2304.14609] Downloading 2304.14609v1...\n",
            "  [2304.14608] Extracted & cleaned: 2304.14608v2 (33 files removed)\n",
            "  [2304.14609] Extracted & cleaned: 2304.14609v1 (15 files removed)\n",
            "✓ [2304.14608] COMPLETED (2/2 versions)\n",
            "[2304.14608] Fetching references...\n",
            "  [2304.14609] Downloading 2304.14609v2...\n"
          ]
        }
      ],
      "source": [
        "%load_ext memory_profiler\n",
        "\n",
        "# Import your main.py file as a module\n",
        "import main\n",
        "import importlib\n",
        "\n",
        "importlib.reload(main)\n",
        "\n",
        "print(\"--- START MEASURING PEAK RAM OF MAIN ---\")\n",
        "%memit main.main()\n",
        "print(\"--- PEAK RAM OF MAIN MEASUREMENT END ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpMiyG9EQBoR"
      },
      "outputs": [],
      "source": [
        "print(\"--- COLAB DRIVE OVERVIEW ---\")\n",
        "!df -h /\n",
        "\n",
        "print(\"\\n--- REQUIRED OUTPUT CAPACITY ---\")\n",
        "# Measure output folder size\n",
        "!du -sh ./ArXivPapers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mount Google Drive and save outputs\n",
        "\n",
        "If you are running this notebook on Google Colab and want to persist the generated output folder `./ArXivPapers` to your Google Drive, run the cell below.\n",
        "\n",
        "Steps:\n",
        "1. Run the code cell and follow the authorization link to mount your Drive.\n",
        "2. The cell will copy (merge) the contents of `./ArXivPapers` to `MyDrive/ArXivPapers`.\n",
        "3. Check your Drive (https://drive.google.com/drive/my-drive) to confirm files are copied.\n",
        "\n",
        "Note: Running this will not delete your Drive files; existing files may be overwritten if names conflict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # If you are running on google colab and want to save data to Drive to avoid losing data when Colab disconnection\n",
        "\n",
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# !zip -r ArXivPapers.zip /content/ArXivPapers\n",
        "\n",
        "# !cp ArXivPapers.zip /content/drive/MyDrive/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "min_ds-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
