{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOD5baC9Pz39",
        "outputId": "6e4638d1-f6d1-479a-ffc4-e6c33fce78db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: arxiv in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (2.3.0)\n",
            "Requirement already satisfied: requests in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (2.32.5)\n",
            "Requirement already satisfied: psutil in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (7.1.0)\n",
            "Requirement already satisfied: memory_profiler in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (0.61.0)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (from arxiv) (6.0.12)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: sgmllib3k in d:\\miniconda3\\envs\\min_ds-env\\lib\\site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install arxiv requests psutil memory_profiler\n",
        "# !apt-get install -y file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0l5xvk3P7el",
        "outputId": "670bd2da-6e06-4830-fba0-a4e53b79ccd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing arxiv_crawler.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile arxiv_crawler.py\n",
        "import arxiv\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import tarfile\n",
        "import shutil\n",
        "import subprocess\n",
        "import gzip\n",
        "\n",
        "SAVE_DIR = \"./ArXivPapers\"\n",
        "\n",
        "\n",
        "def detect_and_fix_filetype(tar_path):\n",
        "    \"\"\"Detect file type using 'file' command or fallback to extension.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run([\"file\", tar_path], capture_output=True, text=True, errors='ignore')\n",
        "        output = result.stdout.strip()\n",
        "    except FileNotFoundError:\n",
        "        # Fallback: check extension\n",
        "        if tar_path.endswith('.tar.gz') or tar_path.endswith('.tgz'):\n",
        "            return \"tar.gz\", None\n",
        "        elif tar_path.endswith('.gz'):\n",
        "            return \"gz\", None\n",
        "        return \"unknown\", None\n",
        "    except Exception:\n",
        "        return \"unknown\", None\n",
        "\n",
        "    if \"PDF document\" in output:\n",
        "        return \"pdf\", None\n",
        "    elif \"gzip compressed data\" in output:\n",
        "        match = re.search(r', was \"([^\"]+)\"', output)\n",
        "        if match:\n",
        "            return \"gz\", os.path.basename(match.group(1))\n",
        "        else:\n",
        "            return \"tar.gz\", None\n",
        "    elif \"tar archive\" in output:\n",
        "        return \"tar.gz\", None\n",
        "    else:\n",
        "        return \"unknown\", None\n",
        "\n",
        "\n",
        "def extract_and_clean(tar_path, dest_folder, base_name):\n",
        "    \"\"\"Extract and clean archive, keeping only .tex and .bib files.\"\"\"\n",
        "    filetype, orig_name = detect_and_fix_filetype(tar_path)\n",
        "    extract_path = os.path.join(dest_folder, base_name)\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "    if filetype == \"pdf\":\n",
        "        return (True, 0)\n",
        "    if filetype == \"unknown\":\n",
        "        return (False, 0)\n",
        "\n",
        "    try:\n",
        "        if filetype == \"tar.gz\":\n",
        "            with tarfile.open(tar_path, 'r:*') as tar:\n",
        "                tar.extractall(path=extract_path)\n",
        "        elif filetype == \"gz\":\n",
        "            out_name = orig_name or f\"{base_name}.file\"\n",
        "            out_path = os.path.join(extract_path, out_name)\n",
        "            with gzip.open(tar_path, 'rb') as fin, open(out_path, 'wb') as fout:\n",
        "                shutil.copyfileobj(fin, fout)\n",
        "    except Exception:\n",
        "        shutil.rmtree(extract_path, ignore_errors=True)\n",
        "        return (False, 0)\n",
        "\n",
        "    deleted = 0\n",
        "    for root, _, files in os.walk(extract_path):\n",
        "        for f in files:\n",
        "            if not f.lower().endswith(('.tex', '.bib')):\n",
        "                try:\n",
        "                    os.remove(os.path.join(root, f))\n",
        "                    deleted += 1\n",
        "                except:\n",
        "                    pass\n",
        "    return (True, deleted)\n",
        "\n",
        "\n",
        "def crawl_single_paper(arxiv_id, save_dir=SAVE_DIR):\n",
        "    \"\"\"Download and process a single arXiv paper with all its versions.\"\"\"\n",
        "    if '.' not in arxiv_id:\n",
        "        return False\n",
        "\n",
        "    client = arxiv.Client()\n",
        "    prefix, suffix = arxiv_id.split('.')\n",
        "    paper_folder = os.path.join(save_dir, f\"{prefix}-{suffix}\")\n",
        "    tex_folder = os.path.join(paper_folder, \"tex\")\n",
        "    os.makedirs(tex_folder, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        search = arxiv.Search(id_list=[arxiv_id])\n",
        "        base_paper = next(client.results(search))\n",
        "        match = re.search(r'v(\\d+)$', base_paper.entry_id)\n",
        "        latest_version = int(match.group(1)) if match else 1\n",
        "    except (StopIteration, Exception):\n",
        "        return False\n",
        "\n",
        "    title = base_paper.title\n",
        "    authors = [a.name for a in base_paper.authors]\n",
        "    abstract = base_paper.summary\n",
        "    submission_date = base_paper.published.strftime(\"%Y-%m-%d\") if base_paper.published else None\n",
        "    publication_venue = base_paper.journal_ref if base_paper.journal_ref else None\n",
        "    categories = base_paper.categories\n",
        "    revised_dates = []\n",
        "\n",
        "    if latest_version > 1:\n",
        "        for v in range(2, latest_version + 1):\n",
        "            try:\n",
        "                vid = f\"{arxiv_id}v{v}\"\n",
        "                search_v = arxiv.Search(id_list=[vid])\n",
        "                paper_v = next(client.results(search_v))\n",
        "                revised_dates.append(paper_v.updated.strftime(\"%Y-%m-%d\") if paper_v.updated else None)\n",
        "            except:\n",
        "                revised_dates.append(None)\n",
        "\n",
        "    pdf_url = base_paper.pdf_url\n",
        "\n",
        "    metadata = {\n",
        "        \"arxiv_id\": arxiv_id.replace('.', '-'),\n",
        "        \"paper_title\": title,\n",
        "        \"abstract\": abstract,\n",
        "        \"authors\": authors,\n",
        "        \"submission_date\": submission_date,\n",
        "        \"revised_dates\": revised_dates,\n",
        "        \"publication_venue\": publication_venue,\n",
        "        \"latest_version\": latest_version,\n",
        "        \"categories\": categories,\n",
        "        \"pdf_url\": pdf_url\n",
        "    }\n",
        "\n",
        "    metadata_path = os.path.join(paper_folder, \"metadata.json\")\n",
        "    try:\n",
        "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(metadata, f, indent=4, ensure_ascii=False)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "    versions_processed = 0\n",
        "    for v in range(1, latest_version + 1):\n",
        "        version_id = f\"{arxiv_id}v{v}\"\n",
        "        version_folder_name = f\"{prefix}-{suffix}v{v}\"\n",
        "        temp_tar = os.path.join(paper_folder, f\"{version_id}.tar.gz\")\n",
        "\n",
        "        try:\n",
        "            search_v = arxiv.Search(id_list=[version_id])\n",
        "            paper_v = next(client.results(search_v))\n",
        "            paper_v.download_source(dirpath=paper_folder, filename=f\"{version_id}.tar.gz\")\n",
        "\n",
        "            success, _ = extract_and_clean(temp_tar, tex_folder, version_folder_name)\n",
        "            if success:\n",
        "                versions_processed += 1\n",
        "\n",
        "            try:\n",
        "                os.remove(temp_tar)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            time.sleep(0.3)\n",
        "\n",
        "        except (StopIteration, Exception):\n",
        "            continue\n",
        "\n",
        "    return versions_processed > 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QApGhns2P8U8",
        "outputId": "8636af08-5546-4b71-b94e-aadeb1968078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing reference_extractor.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile reference_extractor.py\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def format_arxiv_id_for_key(arxiv_id):\n",
        "    \"\"\"\n",
        "    Convert arXiv ID to folder format (yymm-nnnnn).\n",
        "    Examples:\n",
        "        \"2305.04793\" -> \"2305-04793\"\n",
        "        \"2305.04793v1\" -> \"2305-04793\"\n",
        "    \"\"\"\n",
        "    # Remove version suffix if present\n",
        "    clean_id = re.sub(r'v\\d+$', '', arxiv_id)\n",
        "    # Replace dot with dash\n",
        "    return clean_id.replace('.', '-')\n",
        "\n",
        "\n",
        "def get_paper_references(arxiv_id, delay=3, max_retries=5):\n",
        "    \"\"\"Fetch references from Semantic Scholar API with retries.\"\"\"\n",
        "    clean_id = re.sub(r'v\\d+$', '', arxiv_id)\n",
        "    url = f\"https://api.semanticscholar.org/graph/v1/paper/arXiv:{clean_id}\"\n",
        "    params = {\n",
        "        \"fields\": \"references,references.title,references.authors,references.year,references.venue,references.externalIds,references.publicationDate\"\n",
        "    }\n",
        "\n",
        "    API_KEY = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
        "    headers = {}\n",
        "    if API_KEY:\n",
        "        headers[\"x-api-key\"] = API_KEY\n",
        "\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            response = requests.get(url, params=params, headers=headers, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                references = data.get(\"references\", [])\n",
        "                return references, len(references) if references else 0\n",
        "            elif response.status_code == 404:\n",
        "                return None, 0\n",
        "            elif response.status_code == 429:\n",
        "                time.sleep(delay)\n",
        "                retries += 1\n",
        "            else:\n",
        "                time.sleep(delay)\n",
        "                retries += 1\n",
        "        except requests.exceptions.RequestException:\n",
        "            time.sleep(delay)\n",
        "            retries += 1\n",
        "\n",
        "    return None, 0\n",
        "\n",
        "\n",
        "def convert_to_references_dict(references):\n",
        "    \"\"\"Convert Semantic Scholar references to dict format.\"\"\"\n",
        "    result = {}\n",
        "\n",
        "    for ref in references:\n",
        "        if not ref:\n",
        "            continue\n",
        "\n",
        "        external_ids = ref.get(\"externalIds\", {}) or {}\n",
        "        arxiv_id = external_ids.get(\"ArXiv\", \"\")\n",
        "\n",
        "        if not arxiv_id:\n",
        "            continue\n",
        "\n",
        "        key = format_arxiv_id_for_key(arxiv_id)\n",
        "        authors_list = ref.get(\"authors\", [])\n",
        "        authors = [author.get(\"name\", \"\") for author in authors_list if author.get(\"name\")]\n",
        "\n",
        "        publication_date = ref.get(\"publicationDate\", \"\")\n",
        "        year = ref.get(\"year\")\n",
        "        if not publication_date and year:\n",
        "            publication_date = f\"{year}-01-01\"\n",
        "\n",
        "        metadata = {\n",
        "            \"paper_title\": ref.get(\"title\", \"\"),\n",
        "            \"authors\": authors,\n",
        "            \"submission_date\": publication_date if publication_date else \"\",\n",
        "            \"semantic_scholar_id\": ref.get(\"paperId\")\n",
        "        }\n",
        "\n",
        "        result[key] = metadata\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def extract_references_for_paper(arxiv_id, save_dir=\"./ArXivPapers\"):\n",
        "    \"\"\"Extract references for a paper and save to references.json.\"\"\"\n",
        "    paper_id_key = format_arxiv_id_for_key(arxiv_id)\n",
        "    paper_folder = os.path.join(save_dir, paper_id_key)\n",
        "\n",
        "    if not os.path.exists(paper_folder):\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        json_path = os.path.join(paper_folder, \"references.json\")\n",
        "        references, total_found = get_paper_references(arxiv_id)\n",
        "\n",
        "        if references is None or total_found == 0:\n",
        "            with open(json_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump({}, f, indent=4, ensure_ascii=False)\n",
        "            return False\n",
        "\n",
        "        references_dict = convert_to_references_dict(references)\n",
        "        if not references_dict:\n",
        "            with open(json_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump({}, f, indent=4, ensure_ascii=False)\n",
        "            return False\n",
        "\n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(references_dict, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoKzTzp_P-V3",
        "outputId": "554e0335-1cff-41b5-c968-3e0888ae3e8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import psutil\n",
        "import threading\n",
        "import sys\n",
        "import requests\n",
        "import signal\n",
        "import pathlib\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from threading import Lock\n",
        "from arxiv_crawler import crawl_single_paper\n",
        "from reference_extractor import extract_references_for_paper\n",
        "\n",
        "\n",
        "# Global statistics\n",
        "stats_lock = Lock()\n",
        "stats = {\n",
        "    \"total_processed\": 0,\n",
        "    \"both_success\": 0,\n",
        "    \"only_crawler_fail\": 0,\n",
        "    \"no_references\": 0,\n",
        "    \"both_failed\": 0,\n",
        "}\n",
        "\n",
        "monitor_running = True\n",
        "ram_samples_bytes = []\n",
        "peak_disk_usage_bytes = 0\n",
        "\n",
        "def _monitor_resources(baseline_ram, baseline_disk, sleep_interval=2):\n",
        "    \"\"\"Monitor RAM and disk usage in background.\"\"\"\n",
        "    global ram_samples_bytes, peak_disk_usage_bytes, monitor_running\n",
        "\n",
        "    ram_samples_bytes = []\n",
        "    peak_disk_usage_bytes = 0\n",
        "\n",
        "    while monitor_running:\n",
        "        try:\n",
        "            current_ram = psutil.virtual_memory().used\n",
        "            ram_above_baseline = current_ram - baseline_ram\n",
        "            ram_samples_bytes.append(ram_above_baseline)\n",
        "\n",
        "            try:\n",
        "                current_disk = shutil.disk_usage('/').used\n",
        "            except OSError:\n",
        "                current_disk = shutil.disk_usage(pathlib.Path.cwd().anchor).used\n",
        "            disk_above_baseline = current_disk - baseline_disk\n",
        "\n",
        "            if disk_above_baseline > peak_disk_usage_bytes:\n",
        "                peak_disk_usage_bytes = disk_above_baseline\n",
        "\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        time.sleep(sleep_interval)\n",
        "\n",
        "def _print_custom_resource_report(disk_start, disk_end):\n",
        "    \"\"\"Print resource usage report.\"\"\"\n",
        "    global ram_samples_bytes, peak_disk_usage_bytes\n",
        "\n",
        "    avg_ram_bytes = sum(ram_samples_bytes) / len(ram_samples_bytes) if ram_samples_bytes else 0\n",
        "    peak_ram_bytes = max(ram_samples_bytes) if ram_samples_bytes else 0\n",
        "    final_disk_bytes = disk_end - disk_start\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RESOURCE REPORT\")\n",
        "    print(f\"  Average RAM: {avg_ram_bytes / (1024**2):.2f} MB\")\n",
        "    print(f\"  Peak RAM: {peak_ram_bytes / (1024**2):.2f} MB\")\n",
        "    print(f\"  Peak Disk: {peak_disk_usage_bytes / (1024**2):.2f} MB\")\n",
        "    print(f\"  Final Disk: {final_disk_bytes / (1024**2):.2f} MB\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "def process_paper(arxiv_id, save_dir=\"./ArXivPapers\"):\n",
        "    \"\"\"Process a single paper: crawl data first, then extract references.\"\"\"\n",
        "    try:\n",
        "        crawler_success = crawl_single_paper(arxiv_id, save_dir)\n",
        "        references_success = False\n",
        "        \n",
        "        if crawler_success:\n",
        "            references_success = extract_references_for_paper(arxiv_id, save_dir)\n",
        "\n",
        "        with stats_lock:\n",
        "            stats[\"total_processed\"] += 1\n",
        "            if crawler_success and references_success:\n",
        "                stats[\"both_success\"] += 1\n",
        "            elif not crawler_success:\n",
        "                stats[\"only_crawler_fail\"] += 1\n",
        "            if not references_success:\n",
        "                stats[\"no_references\"] += 1\n",
        "\n",
        "        return arxiv_id, crawler_success, references_success\n",
        "\n",
        "    except Exception as e:\n",
        "        with stats_lock:\n",
        "            stats[\"total_processed\"] += 1\n",
        "            stats[\"both_failed\"] += 1\n",
        "        return arxiv_id, False, False\n",
        "\n",
        "\n",
        "def check_paper_exists(arxiv_id):\n",
        "    \"\"\"Check if paper exists with HEAD request.\"\"\"\n",
        "    url = f\"https://arxiv.org/abs/{arxiv_id}\"\n",
        "    try:\n",
        "        response = requests.head(url, timeout=5, allow_redirects=True)\n",
        "        time.sleep(0.3)\n",
        "        return response.status_code == 200\n",
        "    except requests.RequestException:\n",
        "        time.sleep(0.3)\n",
        "        return False\n",
        "\n",
        "\n",
        "def find_last_valid_id(prefix, start_id, jump1=50, back1=10, jump2=5, back2=1):\n",
        "    \"\"\"Find last valid ID using binary search strategy.\"\"\"\n",
        "    try:\n",
        "        start_id = int(start_id)\n",
        "        jump1 = int(jump1)\n",
        "        back1 = int(back1)\n",
        "        jump2 = int(jump2)\n",
        "        back2 = int(back2)\n",
        "    except ValueError:\n",
        "        return 0\n",
        "\n",
        "    start_arxiv_id = f\"{prefix}.{start_id:05d}\"\n",
        "    if not check_paper_exists(start_arxiv_id):\n",
        "        return start_id - 1\n",
        "\n",
        "    last_known_good_id = start_id\n",
        "    current_id = start_id + jump1\n",
        "    state = \"JUMP1\"\n",
        "\n",
        "    while True:\n",
        "        arxiv_id = f\"{prefix}.{current_id:05d}\"\n",
        "        exists = check_paper_exists(arxiv_id)\n",
        "\n",
        "        if exists:\n",
        "            last_known_good_id = current_id\n",
        "            if state == \"JUMP1\":\n",
        "                current_id += jump1\n",
        "            elif state == \"BACK1\":\n",
        "                state = \"JUMP2\"\n",
        "                current_id += jump2\n",
        "            elif state == \"JUMP2\":\n",
        "                current_id += jump2\n",
        "            elif state == \"BACK2\":\n",
        "                break\n",
        "        else:\n",
        "            if state == \"JUMP1\":\n",
        "                state = \"BACK1\"\n",
        "                current_id -= back1\n",
        "            elif state == \"BACK1\":\n",
        "                current_id -= back1\n",
        "            elif state == \"JUMP2\":\n",
        "                state = \"BACK2\"\n",
        "                current_id -= back2\n",
        "            elif state == \"BACK2\":\n",
        "                current_id -= back2\n",
        "\n",
        "    return last_known_good_id\n",
        "\n",
        "\n",
        "def generate_paper_ids(start_month, start_id, end_month, end_id, save_dir=\"./ArXivPapers\", resume_file=None):\n",
        "    \"\"\"Generate list of arXiv IDs, optionally excluding already processed ones.\"\"\"\n",
        "    start_year, start_mon = start_month.split('-')\n",
        "    end_year, end_mon = end_month.split('-')\n",
        "    start_prefix = start_year[2:] + start_mon\n",
        "    end_prefix = end_year[2:] + end_mon\n",
        "\n",
        "    paper_ids = []\n",
        "    processed_ids = set()\n",
        "    processed_ids_file = [\n",
        "        name.replace('-', '.')\n",
        "        for name in os.listdir(save_dir)\n",
        "        if os.path.isdir(os.path.join(save_dir, name))\n",
        "        and \"references.json\" in os.listdir(os.path.join(save_dir, name))\n",
        "    ]\n",
        "\n",
        "    processed_ids.update(processed_ids_file)\n",
        "    print(f\"Resuming: skipping {len(processed_ids)} already processed IDs\")\n",
        "\n",
        "    if start_month == end_month:\n",
        "        for i in range(start_id, end_id + 1):\n",
        "            paper_id = f\"{start_prefix}.{i:05d}\"\n",
        "            if paper_id not in processed_ids:\n",
        "                paper_ids.append(paper_id)\n",
        "    else:\n",
        "        last_valid_start_month = find_last_valid_id(start_prefix, start_id)\n",
        "        for i in range(start_id, last_valid_start_month + 1):\n",
        "            paper_id = f\"{start_prefix}.{i:05d}\"\n",
        "            if paper_id not in processed_ids:\n",
        "                paper_ids.append(paper_id)\n",
        "        for i in range(1, end_id + 1):\n",
        "            paper_id = f\"{end_prefix}.{i:05d}\"\n",
        "            if paper_id not in processed_ids:\n",
        "                paper_ids.append(paper_id)\n",
        "\n",
        "    return paper_ids\n",
        "\n",
        "\n",
        "def print_progress_report():\n",
        "    \"\"\"Print current statistics.\"\"\"\n",
        "    with stats_lock:\n",
        "        total = stats['total_processed']\n",
        "        print(f\"\\nProgress: {total} processed | Success: {stats['both_success']} | Crawler fail: {stats['only_crawler_fail']} | No refs: {stats['no_references']} | Errors: {stats['both_failed']}\")\n",
        "\n",
        "\n",
        "def print_final_report():\n",
        "    \"\"\"Print final statistics.\"\"\"\n",
        "    total = stats['total_processed']\n",
        "    if total == 0:\n",
        "        return\n",
        "\n",
        "    both_success_rate = (stats['both_success'] / total * 100)\n",
        "    crawl_fail_rate = (stats['both_failed'] / total * 100)\n",
        "    no_references_rate = (stats['no_references'] / total * 100)\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"FINAL REPORT\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Total processed: {total}\")\n",
        "    print(f\"Both success: {stats['both_success']} ({both_success_rate:.2f}%)\")\n",
        "    print(f\"Crawler fail: {stats['only_crawler_fail']}\")\n",
        "    print(f\"No references: {stats['no_references']} ({no_references_rate:.2f}%)\")\n",
        "    print(f\"Errors: {stats['both_failed']} ({crawl_fail_rate:.2f}%)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "\n",
        "def run_parallel_processing(start_month, start_id, end_month, end_id,\n",
        "                            max_parallels=2, save_dir=\"./ArXivPapers\"):\n",
        "    \"\"\"Run parallel processing of papers.\"\"\"\n",
        "\n",
        "    with stats_lock:\n",
        "        for key in stats:\n",
        "            stats[key] = 0\n",
        "\n",
        "    paper_ids = generate_paper_ids(start_month, start_id, end_month, end_id, save_dir)\n",
        "    total_papers = len(paper_ids)\n",
        "\n",
        "    print(f\"Processing {total_papers} papers with {max_parallels} threads\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_parallels) as executor:\n",
        "        futures = {\n",
        "            executor.submit(process_paper, arxiv_id, save_dir): arxiv_id\n",
        "            for arxiv_id in paper_ids\n",
        "        }\n",
        "\n",
        "        completed = 0\n",
        "        for future in as_completed(futures):\n",
        "\n",
        "            arxiv_id = futures[future]\n",
        "            completed += 1\n",
        "\n",
        "            try:\n",
        "                paper_id, crawler_ok, refs_ok = future.result()\n",
        "                status = \"OK\" if (crawler_ok and refs_ok) else \"PARTIAL\" if crawler_ok else \"FAILED\"\n",
        "                print(f\"[{completed}/{total_papers}] {status} {paper_id}\")\n",
        "\n",
        "                if completed % 10 == 0:\n",
        "                    print_progress_report()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"[{completed}/{total_papers}] ERROR {arxiv_id}: {e}\")\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"\\nCompleted in {elapsed_time:.2f}s ({elapsed_time/total_papers:.2f}s per paper)\" if total_papers > 0 else \"\")\n",
        "    print_final_report()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the entire processing.\"\"\"\n",
        "\n",
        "    START_MONTH = \"2023-04\"\n",
        "    START_ID = 14607\n",
        "    END_MONTH = \"2023-05\"\n",
        "    END_ID = 14596\n",
        "    MAX_PARALLELS = 2\n",
        "    SAVE_DIR = \"./ArXivPapers\"\n",
        "\n",
        "    try:\n",
        "        disk_usage_start = shutil.disk_usage('/').used\n",
        "    except OSError:\n",
        "        disk_usage_start = shutil.disk_usage(pathlib.Path.cwd().anchor).used\n",
        "    ram_usage_start = psutil.virtual_memory().used\n",
        "\n",
        "    global monitor_running\n",
        "    monitor_running = True\n",
        "\n",
        "    monitor_thread = threading.Thread(\n",
        "        target=_monitor_resources,\n",
        "        args=(ram_usage_start, disk_usage_start, 2),\n",
        "        daemon=True\n",
        "    )\n",
        "    monitor_thread.start()\n",
        "\n",
        "    try:\n",
        "        run_parallel_processing(\n",
        "            start_month=START_MONTH,\n",
        "            start_id=START_ID,\n",
        "            end_month=END_MONTH,\n",
        "            end_id=END_ID,\n",
        "            max_parallels=MAX_PARALLELS,\n",
        "            save_dir=SAVE_DIR,\n",
        "        )\n",
        "    finally:\n",
        "        monitor_running = False\n",
        "        monitor_thread.join()\n",
        "\n",
        "        try:\n",
        "            disk_usage_end = shutil.disk_usage('/').used\n",
        "        except OSError:\n",
        "            disk_usage_end = shutil.disk_usage(pathlib.Path.cwd().anchor).used\n",
        "        _print_custom_resource_report(disk_usage_start, disk_usage_end)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qRsP5StkQBA9",
        "outputId": "228d4284-2f42-476f-c76b-d93607ca0bb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The memory_profiler extension is already loaded. To reload it, use:\n",
            "  %reload_ext memory_profiler\n",
            "--- START MEASURING PEAK RAM OF MAIN ---\n",
            "Resuming: skipping 220 already processed IDs\n",
            "Processing 14780 papers with 2 threads\n",
            "[1/14780] OK 2304.14828\n",
            "[2/14780] OK 2304.14824\n",
            "[3/14780] OK 2304.14829\n",
            "[4/14780] OK 2304.14830\n"
          ]
        }
      ],
      "source": [
        "%load_ext memory_profiler\n",
        "\n",
        "# Import your main.py file as a module\n",
        "import main\n",
        "import importlib\n",
        "\n",
        "importlib.reload(main)\n",
        "\n",
        "print(\"--- START MEASURING PEAK RAM OF MAIN ---\")\n",
        "%memit main.main()\n",
        "print(\"--- PEAK RAM OF MAIN MEASUREMENT END ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpMiyG9EQBoR"
      },
      "outputs": [],
      "source": [
        "print(\"--- COLAB DRIVE OVERVIEW ---\")\n",
        "!df -h /\n",
        "\n",
        "print(\"\\n--- REQUIRED OUTPUT CAPACITY ---\")\n",
        "# Measure output folder size\n",
        "!du -sh ./ArXivPapers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mount Google Drive and save outputs\n",
        "\n",
        "If you are running this notebook on Google Colab and want to persist the generated output folder `./ArXivPapers` to your Google Drive, run the cell below.\n",
        "\n",
        "Steps:\n",
        "1. Run the code cell and follow the authorization link to mount your Drive.\n",
        "2. The cell will copy (merge) the contents of `./ArXivPapers` to `MyDrive/ArXivPapers`.\n",
        "3. Check your Drive (https://drive.google.com/drive/my-drive) to confirm files are copied.\n",
        "\n",
        "Note: Running this will not delete your Drive files; existing files may be overwritten if names conflict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # If you are running on google colab and want to save data to Drive to avoid losing data when Colab disconnection\n",
        "\n",
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# !zip -r ArXivPapers.zip /content/ArXivPapers\n",
        "\n",
        "# !cp ArXivPapers.zip /content/drive/MyDrive/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "min_ds-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
